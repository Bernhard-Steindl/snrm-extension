\chapter{State of the Art} \label{chapter:state-of-the-art}

In the chapter State of the Art theoretical aspects of current neural 
    information retrieval models are examined.
Particulary, this analysis focuses on the proposals of the
    Neural Vector Space Model (NVSM) from
    Van Gysel et al. \cite{van-gysel:2017:neural-vector-spaces}
    and obviously due to the topic of the work on the
    Standalone Neural Ranking Model (SNRM) from 
    Zamani et al. \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

\section{Standalone Neural Ranking Model (SNRM)}

Unlike other current neural information retrieval models that 
    are employed for re-ranking the top documents retrieved by 
    a first-stage ranker beforehand,
    Zamani et al. \cite{zamani:2018:from-neural-reranking-to-neural-ranking} 
    propose a standalone neural ranker, that is
    a model that is not dependent on a first-stage ranker.
Their IR model uses neural approaches for learning sparse high-dimensional
    latent vector representations of query and document text 
    with the aim to capture semantic meaning.
The authors emphasize the importance of the representations' sparseness
    to allow an efficient inverted index construction and retrieval.
A deep neural network is used for capturing semantic relationships between
    queries and documents, and by rewarding sparsity in learned representations
    the model's objective is to detect words' semantic meaning and 
    relationships and to compress them to latent vector representations.
The model takes n-grams of either query or document text,
    and then obtains the corresponding word vectors 
    either from a pre-trained embedding, such as GloVe, or 
    from a learned embedding matrix during model training. 
Subsequently, these embedded n-grams resp. sequence of word vectors 
    are mapped by a fully connected neural network 
    to low-dimensional dense representations,
    aiming to extract semantic meaning of words, 
    for dimensionality reduction and for data compression.
Then, SNRM's neural network learns a function to transform 
    these low-dimensional dense representations to 
    high-dimensional sparse vector representations.
Concerning the neural network's architecture, the model's 
    stack of layers looks like an hourglass or like an autoencoder
    regarding its layer dimensions.
The networks input neurons resp. input channels is equal to the
    embedding dimensions (e.g. 300-dimensional).
These input neurons are then connected with three to four 
    fully-connected hidden layers, with decreasing units in the middle 
    layers (e.g. 100 dimensions), and with increasing neurons in 
    the upper layers.
For obtaining high-dimensional representations the network's output 
    layer has a high number of neurons (e.g. 10,000 units).\\
After the model has been trained, latent representations are generated
    by model inference using either a query or a document text.
Under the assumption of sufficient representation sparseness, 
    the authors suggest to be able to efficiently construct an 
    inverted index and to retrieve documents for a given query 
    with the model.
The index can be imagined as a dictionary of 
    the learned representations' dimensions resp. latent terms, 
    where those documents of the collection are assigned to a 
    latent term which seem to be relevant.
That is, the inverted index stores for every latent term a posting list 
    of relevant documents.
A document seems relevant for a certain latent term, if its
    corresponding representation's dimension is positive.\\
During the retrieval phase, a query's respective high-dimensional 
    sparse representation is obtained by model inference.
Then, a query representation's positive dimensions determine
    the latent terms to consider and
    which documents of the inverted index are returned.
Afterwards, a matching function calculates a retrieval score for each
    returned document of a query. 
That is, generaly the model's workflow is comprised of three phases,
    namely of training a model to learn high-dimensional sparse 
    representations of documents and queries,
    to create an inverted index from the document collection
    and subsequently to retrieve documents for a given query.
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}

\subsection*{Training phase}

During training the model tries to learn to generate high-dimensional 
    sparse representation of queries and documents.
A sliding window resp. convolution kernel of width $n$ (e.g. $n=5$)
    in the first hidden layer of the neural network encodes 
    sequences of word vectors as n-grams and tries to capture local 
    relationships between words in the text.
Then, these embedded n-grams are passed through the neural network's
    fully-connected convolution layers to learn sparse representations.
The network's design intends to use Rectified Linear Unit 
    ($ReLU(x)=max\{0,x\}$) as an activation function after a convolution
    layer, which ensures that all negative output values from its layer 
    neurons are replaced with zero, and as a result increasing
    sparsity in the output naturally.
To obtain the final representation of a query or document,
    the output of the neural network is aggregated using average pooling
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

Related to learning representations, the model's further objective is
    to learn which document out of two is more relevant for a given query.
The training of the model is done with batches of training data.
A training data batch $T$ consist of $N$ training instances
    formalized as a four-tuple, where
    $q_i$ denotes a query, 
    $d_{i,1}$ and $d_{i,2}$ two document candidates and
    $y_i \in \{-1,1\}$ serves as a relevance label, indicating which document
    is more relevant, for the the $i$\textsuperscript{th} training instance.
\[
T=\{ (q_1,d_{1,1},d_{1,2},y_1), \ldots, (q_N,d_{N,1},d_{N,2},y_N)\}
\]

Zamani et al. propose to add a hinge loss to the training procedure.
In the following hinge loss equation 
    $\phi_Q(q_i)$ denotes the vector representation of a query $q_i$,
    $\phi_D(d_{i,1})$ and $\phi_D(d_{i,2})$ are document representations 
    of the respective document $d_{i,1}$ and $d_{i,2}$ and
    the matching function $\psi$ is the dot product function of a query and 
    document representation.
\[
L_{h}(q_i, d_{i,1}, d_{i,2}, y_i) = max\bigl(0,1 - y_i * \bigl[ \psi(\phi_Q(q_i), \phi_D(d_{i,1})) - \psi(\phi_Q(q_i), \phi_D(d_{i,2})) \bigr] \bigr)
\]

Further, the SNRM should improve retrieval accuracy by increasing the representation 
    sparsity.
A vector's sparsity is the ratio between its number of zero values to 
    the number of all coefficients in the vector.
For maximizing the sparsity ratio of learned representations,
    the authors suggest minimizing a vector's L1 norm.
\[
L_1(\vec{v}) = \sum_{i=1}^{\left |  \vec{v}\right |} \left |  \vec{v_i}\right |
\]

When combininig the hinge loss and the L1 norm, the model's final loss function 
    for the $i$\textsuperscript{th} training instance is defined as follows,
    where the regularization term $\lambda$ aims at controlling the sparsity
    ratio and $||$ represents tensor concatenation.
\[
L = L_{h}(q_i, d_{i,1}, d_{i,2}, y_i) + \lambda * L_1(\phi_Q(q_i)||\phi_D(d_{i,1})||\phi_D(d_{i,2}))
\]

The training of the model is done by minimizing the loss function and therefore 
    achieving the retrieval and sparsity objective
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

\subsection*{Inverted index construction phase}

\subsection*{Retrieval phase}




