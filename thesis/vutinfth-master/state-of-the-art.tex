\chapter{State of the Art} \label{chapter:state-of-the-art}

In the chapter State of the Art theoretical aspects of current neural 
    information retrieval models are examined.
Particulary, this analysis focuses on the proposals of the
    Neural Vector Space Model (NVSM) from
    Van Gysel et al. \cite{van-gysel:2017:neural-vector-spaces}
    and obviously due to the topic of the work on the
    Standalone Neural Ranking Model (SNRM) from 
    Zamani et al. \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

\section{Standalone Neural Ranking Model (SNRM)}

Unlike other current neural information retrieval models that 
    are employed for re-ranking the top documents retrieved by 
    a first-stage ranker beforehand,
    Zamani et al. \cite{zamani:2018:from-neural-reranking-to-neural-ranking} 
    propose a standalone neural ranker, that is
    a model that is not dependent on a first-stage ranker.
Their IR model uses neural approaches for learning sparse high-dimensional
    latent vector representations of query and document text 
    with the aim to capture semantic meaning.
The authors emphasize the importance of the representations' sparseness
    to allow an efficient inverted index construction and retrieval.
A deep neural network is used for capturing semantic relationships between
    queries and documents, and by rewarding sparsity in learned representations
    the model's objective is to detect words' semantic meaning and 
    relationships and to compress them to latent vector representations.\\
During training, the model takes n-grams of query and document text,
    and then obtains the corresponding word vectors 
    either from a pre-trained embedding, such as GloVe, or 
    from a learned embedding matrix. 
Subsequently, these embedded n-grams resp. sequence of word vectors 
    are mapped by a fully connected neural network 
    to low-dimensional dense representations,
    aiming to extract semantic meaning of words, 
    for dimensionality reduction and for data compression.
Then, SNRM's neural network learns a function to transform 
    these low-dimensional dense representations to 
    high-dimensional sparse vector representations.\\
Concerning the neural network's architecture, the model's 
    stack of layers looks like an hourglass or like an autoencoder
    regarding its layer dimensions.
The networks input neurons resp. input channels is equal to the
    embedding dimensions (e.g. 300-dimensional).
These input neurons are then connected with three to four 
    fully-connected hidden layers, with decreasing units in the middle 
    layers (e.g. 100 dimensions), and with increasing neurons in 
    the upper layers.
For obtaining high-dimensional representations the network's output 
    layer has a high number of neurons (e.g. 10,000 units).\\
After the model has been trained, latent representations are generated
    by model inference using either a query or a document text.
Under the assumption of sufficient representation sparseness, 
    the authors suggest to be able to efficiently construct an 
    inverted index and to retrieve documents for a given query 
    with the model.
The index can be imagined as a dictionary of 
    the learned representations' dimensions resp. latent terms, 
    where those documents of the collection are assigned to a 
    latent term which seem to be relevant.
That is, the inverted index stores for every latent term a posting list 
    of relevant documents.
A document seems relevant for a certain latent term, if its
    corresponding representation's dimension is positive.\\
During the retrieval phase, a query's respective high-dimensional 
    sparse representation is obtained by model inference.
Then, a query representation's positive dimensions determine
    the latent terms to consider and
    which documents of the inverted index are returned.
Afterwards, a matching function calculates a retrieval score for each
    returned document of a query. 
That is, generally the model's workflow is comprised of three phases,
    namely of training a model to learn high-dimensional sparse 
    representations of documents and queries,
    to create an inverted index from the document collection
    and subsequently to retrieve documents for a given query.
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}

\subsection*{Training phase}

During training the model tries to learn to generate high-dimensional 
    sparse representation of queries and documents.
A sliding window resp. convolution kernel of width $n$ (e.g. $n=5$)
    in the first hidden layer of the neural network encodes 
    sequences of word vectors as n-grams and tries to capture local 
    relationships between words in the text.
Then, these embedded n-grams are passed through the neural network's
    fully-connected convolution layers to learn sparse representations.
The network's design intends to use Rectified Linear Unit 
    ($ReLU(x)=max\{0,x\}$) as an activation function after a convolution
    layer, which ensures that all negative output values from its layer 
    neurons are replaced with zero, and as a result increasing
    sparsity in the output naturally.
To obtain the final representation of a query or document,
    the output of the neural network is aggregated using average pooling
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

Related to learning representations, the model's further objective is
    to learn which document out of two is more relevant for a given query.
The training of the model is done with batches of training data.
A training data batch $T$ consist of $N$ training instances
    formalized as a four-tuple, where
    $q_i$ denotes a query, 
    $d_{i,1}$ and $d_{i,2}$ two document candidates and
    $y_i \in \{-1,1\}$ serves as a relevance label, indicating which document
    is more relevant, for the the $i$\textsuperscript{th} training instance.
\[
T=\{ (q_1,d_{1,1},d_{1,2},y_1), \ldots, (q_N,d_{N,1},d_{N,2},y_N)\}
\]

Zamani et al. propose to add a hinge loss to the training procedure.
In the following hinge loss equation 
    $\phi_Q(q_i)$ denotes the vector representation of a query $q_i$,
    $\phi_D(d_{i,1})$ and $\phi_D(d_{i,2})$ are document representations 
    of the respective document $d_{i,1}$ and $d_{i,2}$ and
    the matching function $\psi$ is the dot product function of a query and 
    document representation.
\[
L_{h}(q_i, d_{i,1}, d_{i,2}, y_i) = max\bigl(0,1 - y_i * \bigl[ \psi(\phi_Q(q_i), \phi_D(d_{i,1})) - \psi(\phi_Q(q_i), \phi_D(d_{i,2})) \bigr] \bigr)
\]

Further, the SNRM should improve retrieval accuracy by increasing the representation 
    sparsity.
A vector's sparsity is the ratio between its number of zero values to 
    the number of all coefficients in the vector.
For maximizing the sparsity ratio of learned representations,
    the authors suggest minimizing a vector's L1 norm.
\[
L_1(\vec{v}) = \sum_{i=1}^{\left |  \vec{v}\right |} \left |  \vec{v_i}\right |
\]

When combininig the hinge loss and the L1 norm, the model's final loss function 
    for the $i$\textsuperscript{th} training instance is defined as follows,
    where the regularization term $\lambda$ aims at controlling the sparsity
    ratio and $||$ represents tensor concatenation.
\[
L = L_{h}(q_i, d_{i,1}, d_{i,2}, y_i) + \lambda * L_1(\phi_Q(q_i)||\phi_D(d_{i,1})||\phi_D(d_{i,2}))
\]

The training of the model is done by minimizing the loss function and therefore 
    achieving the retrieval and sparsity objective
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

\subsection*{Inverted index construction phase}

After finishing the SNRM's training, an inverted index is constructed
    from the document collection.
Each document is served as an input to the trained model, to retrieve
    the respective high-dimensional document representation.
That is, the tokenized document text is translated to word vectors
    using an embedding layer, and then passes the trained 
    neural network.
Every index resp. latent term of a representation that is non-zero
    is considered as a latent topic or concept that this document describes.
An inverted index saves for all latent terms, that is for all dimensions
    of a representation, those documents that are considered relevant.
So, if a document representation's index $i$ has a positve value,
    this document is added to the inverted index at index $i$.
The memory efficiency of a constructed inverted index depends on the
    sparsity of the obtained document representations
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.

\subsection*{Retrieval phase}

Subsequently, for retrieval an embedded query text is fed into the trained
    model to obtain its corresponding query representation.
For every non-zero coefficient in the query representation the
    documents in the inverted index are fetched with the corresponding
    representation index.
Then, a retrieval score for a query and a document is calculated,
    by multiplying every non-zero coefficient of the query with the corresponding
    document respresentation coefficient, followed by a summation
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.
\[
Retrieval\_score(\vec{q}, \vec{d}) = \sum_{\vec{q_i} > 0} \vec{q_i} * \vec{d_i}
\]

After having obtained the retrieval scores for all document candidates,
    the top documents sorted by the retrieval score are returned as 
    answer to a given query.

\section{Neural Vector Space Model (NVSM)}

As the name of the model suggests, NVSM is also an information
    retrieval model that makes use of a shallow neural network.
Unlike SNRM, which utilizes supervised learning, the 
    Neural Vector Space Model learns latent representations
    of words and documents and a
    transformation function between their corresponding 
    vector spaces
    in an unsupervised manner.
That is, the model learns from the documents in the document 
    collection without any query-document relevance labels, 
    without feature engineering and
    without using a pre-trained word embedding.
Although the model operates in latent vector space,
    the dimensionality of the learned word and document
    repsentations is differeent.
Hence, a transformation function between these vector spaces
    is also learned by the model.\\
Van Gysel et al. emphasize that their model learns based on
    three fundamental information retrieval regularities, 
    which are part of the model's optimization objectives.
Firstly, words that occur in many documents are not discriminatory
    and should therefore be neglected (referred as term specificity).
Further, document representations in latent space that have 
    other representations nearby, tend to describe
    similar or related concepts (cluster hypothesis).
Lastly, word phrases carry more semantic
    meaning than single words, and therefore representations
    are learned with sequences of words (semantic matching).\\
During document retrieval, a query's text is mapped
    from the word representation space to the document feature
    space and in theory the cosine similarity is used 
    for calculating a retrieval score between a query and
    a document.
However, the authors suggest that in practice this ranking 
    metric can be formulated as an approximate nearest neigbor 
    search problem, so that not all documents of the collection
    have to be ranked. \cite{van-gysel:2017:neural-vector-spaces}

\subsection*{Model design}

A document consists of a sequence of words,
    which represent and characterize a document's content and topic.
It is therefore appropriate to make use of word phrases
    for learning representations.\\
For the NVSM's training process, documents from the collection are
    sampled.
For each document a n-gram, that is a word phrase of $n$ 
    contigous words, is added
    along with with the respective document $d$ as
    training instance $(w_1, \ldots, w_n; d)$ to a batch.
These n-gram/document pairs in the batch are used
    as an input for learning the model's parameters.
The NVSM learns word and document representations,
    which are embedded in matrices $R_V \in \mathbb{R}^{|V| \times k_w}$
    and $R_D \in \mathbb{R}^{|D| \times k_d}$, where 
    $|V|$ and $|D|$ is the size of the vocabulary resp. document collection,
    $k_w$ is the dimension the vector representation of a word and
    $k_d$ the dimesion of a document
Because a word representation $\vec{R}_V^{(w_i)}$,
    and a document representation $\vec{R}_D^{(d_j)}$ are of different
    dimensionality, the neural network also learns a linear
    tranformation $f \colon \mathbb{R}^{k_w} \to \mathbb{R}^{k_d}$ 
    from the word feature space to the document feature space,
    where $W$ is a $k_d \times k_w$ parameter matrix, that is 
    also learned simulateneously by the model.
\[
f(\vec{x}) = W\vec{x}
\]

A learning objective of NVSM is, that n-gram representations are projected
    nearby those document representations, that contain the n-gram.
As stated by the cluster hypothesis, representations tend to be 
    more similar, if they are closer to each other in the feature space.
Van Gysel et al. propose to compose a n-gram word representation by
    averaging its individual word representations as follows:
\[
g(w_1, \ldots, w_n) = \frac{1}{n} \sum_{i+1}^{n} \vec{R}_V^{(w_i)}
\]

The authors illustrate the projection of a query $q$ to the document
    feature space by function composition of $f$ and $g$: $(f \circ g)(q)$.
They suggest to use the resulting n-gram query representation in the document 
    feature space for calculating a certain document's matching score
    with the cosine similarity or for using approximate 
    nearest neighbor search to find close documents in the 
    feature space, in practice.\cite{van-gysel:2017:neural-vector-spaces}

\subsection*{Model objectives}

In order for the NVSM to fufill the three above-mentioned regularities
    term specificity, cluster hypothesis and semantic matching,
    the authors explain in their paper in detail how they optimized 
    the model with respect to these objectives.

Regarding semantic matching, the authors use the prior defined function $g$, 
    for generating n-gram word representations,
    so that the model learns similarity and semantic relations
    between an unordered sequence of words.
Word phrases that are specific and more descriptive of a document's concepts, 
    should also be more reflected in the n-gram represenation.
Otherwise, more frequent words in documents should have smaller effect on 
    representations.
They address term specificity in the model's objectives by using the 
    function $g$ together with the L2 normalization 
    of the n-gram representation vector, which is supposed to 
    emphasize word specificity in the representations.
\[
norm(\vec{x}) = \frac{\vec{x}}{\|\vec{x}\|}
\]

Because ranking is done using nearest neighbor search, 
    documents of similar topics should cluster together in feature space.
In addition, n-gram representations should be projected near those 
    documents that contain the respective n-grams for retrieval.
Let $B_i^{(p)}$ denote the n-gram and $B_i^{(d)}$ the respective document 
    of the $i$\textsuperscript{th} batch instance.
The projection of a word sequence into the $k_d$ dimensional document
    feature space is further enhanced with the L2 normalization.
Van Gysel et al. also standardize the projection, add a bias vector parameter
    and make use of an activation function to map the projection to a different 
    range.
However, for reasons of simplicity the non-standardized variant is used in the 
    following for illustrating the projection of a n-gram into the document 
    feature space.
\[
\widetilde{T}\Big(B_i^{(p)}\Big) = (f \circ norm \circ g) \Big(B_i^{(p)} \Big)
\]

According to Van Gysel et al., the third objective, 
    namely that n-grams are projected near related document representations 
    in the document feature space, is achieved by maximizing the similarity between 
    the projection of the n-gram $B_i^{(p)}$ and the learned 
    document representation of the respective document $B_i^{(d)}$,
    that is to
    \[
    \textrm{maximize similarity of }\widetilde{T}\Big(B_i^{(p)}\Big) \textrm{ and } \vec{R}_D^{\big(B_i^{(d)}\big)}
    \]
    while minimizing the similarity between the projected n-gram representation
    \[
    \widetilde{T}\Big(B_i^{(p)}\Big)
    \]
    and the representation of other documents.

The authors of NVSM's paper formulate the model's objectives and
    the parameter estimation using a loss function 
    that is optimized with stochastic gradient descent (SGD)
    and the Adam.
    \cite{van-gysel:2017:neural-vector-spaces}



