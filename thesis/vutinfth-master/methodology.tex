\chapter{Methodology}

The practical values of the bachelor thesis will be the reproducibility of the SNRM implementation and the migration of the original SNRM source code to be executable with the latest versions of the machine learning library PyTorch \footnote{PyTorch website \url{https://pytorch.org}}  

An analysis \cite{he:2019:state-of-ml-frameworks} from Horace He shows that in 2019 the machine learning frameworks PyTorch and TensorFlow \footnote{TensorFlow website \url{https://www.tensorflow.org}} are the most popular used ones.
According to Horace He, TensorFlow is still the dominant framework in the industry by comparing job listings, for instance.
However, he also studied research papers that either use PyTorch or TensorFlow and found PyTorch has gained traction since 2018 and actually holds the majority in the research community at the moment. \cite{he:2019:state-of-ml-frameworks}
The growing popularity of using PyTorch motivates the usage of the framework for the bachelor thesis.

The theoretical values of the bachelor thesis will be a comparison of the original SNRM and the ported SNRM, as well as an analysis of state of the art neural information retrieval models, like SNRM and NVSM.
\\Concerning the practical part of the thesis, the first goal is to be able to run the original SNRM.
For training and evaluation of the IR model the Microsoft MAchine Reading COmprehension Dataset (MS MARCO \footnote{MS MARCO website \url{http://www.msmarco.org}}) for passage ranking (MSMARCO-Passage-Ranking) will be used.
One of the first steps to approach this goal is to find compatible versions for Python \footnote{Python website \url{https://www.python.org}}, TensorFlow and NumPy \footnote{Numpy website \url{https://www.numpy.org}}, because the originally used legacy package versions of the authors are not listed on the GitHub page of SNRM \footnote{Hamed Zamani's SNRM github repository \url{https://github.com/hamed-zamani/snrm}}.
Next up, will be the implementation of missing Python functions in the original SNRM code, comprising among others the creation of an in-memory term dictionary, and the provision of training and evaluation batch data to the model.
Afterwards, the original SNRM will be ported resp. migrated to SNRM-PyTorch, which will be a SNRM with up-to-date versions of Python, PyTorch and NumPy.
For version control of the source code a private GitHub repository, forked from the original SNRM, will be used.
The design of the implementation and the source code will be available in the thesis chapter Design and Implementation.

For the theoretical part, a comparison as well as an evaluation of the model training performance and ranking results of the original SNRM and SNRM-PyTorch with the MS MARCO dataset will be presented in the chapter Evaluation.

The bachelor thesis chapter State of the Art will be a comparison of SNRM to other recent proposals of Neural Information Retrieval models, like the Neural Vector Space Model (NVSM).



Actually done:

Read papers of SNRM, NVSM, An Introduction to Neural Information Retrieval,
first two weeks of Coursera Stanford Machine Learning Course \footnote{Coursera Stanford Machine Learning Course website \url{https://www.coursera.org/learn/machine-learning}}
useful for learning about an Introduction to machine learning, supervised and unsupervised learning, intuition of
cost function, gradient descent, linear regression, refresher course in linear algebra (matrices, vectors)
Python tutorial \footnote{Python tutorial website \url{https://docs.python.org/3.6/tutorial/}}

Understand SNRM Code and Tensorflow, NumPy, PyTorch
Gain experience in using pip and Conda for dependency resp. package management, as well as IDE Visual Studio Code.


Book François Chollet - Deep Learning with Python

According to the paper of SNRM, as datasets were used Robust (TREC), ClueWeb und AOL query logs and a Wikipedia dump.
However, 


Google Colab like a Jupyter Notebook but with free NVIDA GPU

SNRM original tensorflow code make runnable on local MacBook and CPU
Extension of original SNRM code in Tensorflow to be able to use the MS MARCO passage ranking dataset.

Unfortunately, the authors neither stated the versions of Python, Tensorflow and NumPy on their paper nor in the SNRM GitHub repository.
Hence, the first effort was put in trying different versions and resolve version conflicts between dependencies.
By trial-and-error executing SNRM Tensorflow with various versions, the following versions seemed to make the application run.
Probable, these versions were used by the authors of he SNRM with Tensorflow:

\begin{itemize}
    \item python 3.6.9
    \item tensorflow 1.4.0
    \item numpy 1.16.4
\end{itemize}

After finding potential suitable Python, Tensorflow and Numpy versions, runtime errors occured, likely due to invokations of incompatible 
    resp. obsolete api methods and attributes by the SNRM code.



triples.train.tsv (Dataset Advanced IR/MS MARCO), NLTK \verb|word_tokenize|




code/dictionary.py




SNRM's \texttt{code/dictionary.py} opens a file comprising tokens and generates two datastructures, one for mapping from an id to a token, 
    and vice versa.
According to the original SNRM source code, the authors loaded vocabulary terms from the output of Galago's \verb|dump_term_stats| function.
There was no reference supplied where to download their vocabulary terms.
Hence, the file \verb|allen_vocab_lower_10/tokens.txt| from the Advanced Information Retrieval course's exercise files 
    \footnote{Advanced IR course's excercise files \url{https://owncloud.tuwien.ac.at/index.php/s/QA4LEtxdBokqdNx}}
    was chosen and used as a token file and in this way the known vocabulary.
The original implementation for loading the vocabulary tokens had to be adjusted due to a different format of the token file.
It is worth to note, that the original SNRM implementation only used simple whitespace (\verb|" "|) tokenization, for demonstrating
    the concept of retrieval with two exemplary hard-coded queries (see \texttt{code/retrieval.py}).
Instead of retaining the simple whitespace tokenization, NLTK word tokenization was employed.
NLTK in version 3.4.5, a Python dependency, was added for using the NLTK corpus' english stopwords and word tokenization.
The file \texttt{code/dictionary.py} was edited and the function function \verb|get_term_id_list| was added as a 
    replacement of the previous tokenization technique, for the means to generate a list of term-ids from 
    query and document text via word tokenization with regards to remove stop words, too.



% Die Methode zum Generieren von Data-Batches (generate_batches) war im Original-Code noch nicht umgesetzt. 
% Ich vermute, weil diese abhängig vom gewählten Datensatz ist. Als Datensatz habe ich mal die train.triples.tsv von dir genommen. 
% Ich bin mir aber unsicher, ob mein Batch dem entspricht, was erwartet wird, weil ich das Kommentar, das die Nachbedingung beschreibt, 
% nicht ganz verstehe. Für die Tokenization v. query und passages hab ich NLTK gefunden und vorerst hinzugefügt (word\_tokenize).




Implementation halted for writing the bachelor thesis proposal, composed of the thesis title, literature, thesis structure and the motivation, 
    and planned goals of the thesis.

The paper's authors did not provide the function \verb|generate_batch| within the python file \verb|code/train.py|, 
    which is supposed to be used for generating a new batch of training data (resp. validating data), while also remembering which 
    data was already generated.
One raw batch is a matrix comprising various query texts, two document texts (passages), and a label indicating which document 
    is more relevant to the corresponding query text.
The batch data's query and document text have to be represented as term ids. 
As a consequence, the positve and negative document as well as the query texts had to be tokenized, stopwords had be removed,
    and the representations had to be trimmed or padded to have a common length.

Concerning training and validation data, at first a processed (HOW?) subset of the MS MARCO dataset was used, 
    \verb|triples.train.tsv| (ca. 1.7 GB), which was downloaded from the Advanced IR course's exercise files.
The triples file was split in two distinct files, namely one for training and the other for validation data.
The format of the dataset is \verb|"query \t relevant-doc \t non-relevant-doc"|.

The SNRM model was operated with pre-trained GloVe word vectors
    \footnote{GloVe pre-trained word vectors GitHub repository \url{https://github.com/stanfordnlp/GloVe\#download-pre-trained-word-vectors}},
    which were downloaded from their GitHub repository.
Different GloVe embeddings were used for experimenation, namely \verb|glove.42B.300d.txt|, 
    of corpora Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download), 
    and \verb|glove.6B.100d.txt|, of corpora Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 100d vectors, 822 MB download).

% google tensorboard
% 'dict_file_name', 'data/tokens/tokens_lowered_2019-12-28_001150_min_10.txt
% google-colab/SNRM_Extension_Steindl.ipynb 

Similarly, the authors of the SNRM paper did not implement the \verb|generate_batch| function for the phase of index construction in 
    the file \verb|code/index_construction.py|. As a result, the function had to be implemented in such a way, that it returns a
    batch resp. matrix comprising of multiple document-id entries and assigning every document-id a list of the document token ids.
    The document token ids were obtained the same way as in the training phase's batch generation 
    (tokenization, stopword removal, padding/trimming).
As the data source for generating passage text batches, the document collection \verb|collection.tsv| (ca. 2.9 GB)
    \footnote{MS MARCO passage ranking document collection dataset download link 
    \url{https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz}}, 
    from the MS MARCO passage ranking GitHub repository was used, which has the format \verb|"passage-id \t passage-text"|.

For the reader it remains unclear, why the source code for batch generation has been deleted and replaced with an exception pointing out the
    missing implementation. 
Probably, the code was too domain-specific or too closely tied to the chosen training data and document collection,
    so it was not worth making it publicly available on GitHub.
Nevertheless, for the reproduction of the SNRM's results it is necessary to add the lacking implementation details.

As already stated, the Python script for the retrieval phase (\texttt{code/retrieval.py}) had only hard-coded two example queries,
    and demonstrated the steps for obtaining the retrieval scores for documents, for a given query.
After mapping the query text to a list of token ids, generation of a latent query representation vector through model inference, 
    inverted index lookup for relevant documents with the latent representation's dimension, a document's retrieval score was
    calculated by summation over a document's latent term weigh and the coefficient of the corresponding latent query term dimension.
Afterwards, the original SNRM code just writes the document retrieval scores to a binary file by Pyhon object serialization (\texttt{pickle}).
One of the goals in reproducing the paper and the source code, was to evaluate the retrieval model with recent evaluation tools like 
    \verb|trec_eval| \footnote{Evaluation software \texttt{trec\_eval} GitHub repository \url{https://github.com/usnistgov/trec_eval}},
    and \verb|msmarco_eval.py| 
    \footnote{Evaluation software \texttt{msmarco\_eval.py} GitHub repository \url{https://github.com/spacemanidol/MSMARCO/blob/master/Ranking/Baselines/msmarco_eval.py}}.
Associated with this, the retrieval Python script was modified, to process as input a query file, to write the retrieval result as an 
    evaluation file comprising the retrieved and relevant document candidates in TREC format, 
    and to allow to stop retrieval after a configurable number of input queries have been processed. 

The MS MARCO passage ranking GitHub repository also provides queries and TREC qrel files for model evaluation.
As a starting point, the query file \texttt{queries.dev.small.tsv} and the qrel file \texttt{qrels.dev.small.tsv} were downloaded
    \footnote{MS MARCO passage ranking queries, and qrel files download link \url{https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz} }.
    and used.

% queries.air-subset.tsv

By writing and utilizing a Python script for analysing the qrel file's content, the following statistics have been observed.
The qrel file \texttt{qrels.dev.small.tsv} comprises of 6,980 queries in total.
When grouping all queries by how much relevant documents a certain query has, as a result is obtained, that 
    out of all queries, 6,590 queries have only one relevant document, resp. 331 queries have two, 
    51 queries have three and 8 queries have four relevant documents, accoding to the relevance judgements file.



Unfortunately, the own available personal computers lack the accessibility to a NVIDIA GPU.
NVIDIA GPUs proved to be well equipped for improving model training performance and speed, compared to a CPU.
Hence, after making the original SNRM code with stated modifications executable, a Google Colab notebook was created
    with Google Colaboratory (Colab) \footnote{Google Colaboratory (Colab) website \url{https://colab.research.google.com}},
    for executing the SNRM application phases in a Jupyter Notebook like environment, 
    but also utilizing a free NVIDIA GPU.
In the Google Colab Notebook, commands were used to setup the platform (NVIDIA CUDA, NVIDIA cuDNN, Miniconda, pip, Python, etc.),
    checkout the GitHub source code,
    install the required project dependencies, downloading and extracting datasets,
    before invoking the Python scripts, according to the model's information retrieval phases
    model training, inverted index construction, retrieval, and evaluation.
Note that to run Tensorflow GPU-accelerated the package \verb|tensorflow-gpu| must also be installed,
    with \verb|pip| for instance.

Due to the authors' use of a relatively old Tensorflow version 1.4.0 nowadays, special effort had to be put in setting up
    NVIDIA cuDNN in version 6 and NVIDIA CUDA version 8, to be compatible with \verb|tensorflow_gpu-1.4.0|.
Google Colab's platform is setup with an already preinstalled version of these NVIDIA software, which was in its original condition
    not compatible with the SNRM source code resp. Tensorflow version 1.4.
Google provides a table of tested build configurations on their Tensorflow website
    \footnote{Tensorflow tested Linux GPU build configurations \url{https://www.tensorflow.org/install/source\#gpu}}, 
    which was useful in finding the supported NVIDIA sofware versions, that has to be installed in Google Colab 
    after deleting the originally preinstalled packages.
Unfortunately, the authors have not disclosed the used NVIDIA CUDA and cuDNN versions.

After writing a Google Colab notebook for SNRM was done, the model training was run for several hours in Google Colab,
    followed by index construction.
Evaluating the model's retrieval results showed that the inverted index was only created for a small document fraction.
This bug could be identified and was fixed soon afterwards by processing the whole document collection or until a
    configurable number of documents has been indexed.

Index construction posed another challenge, because the original source code utilizes an in-memory storage
    for the inverted index and writes it to a file with Python's \texttt{pickle} not until index construction finishes.
Subsequentially, in the retrieval phase, the index is read in from this binary file, that was created prior to that.
Though, the authors noted in their source code, that the inverted index should be implemented and that it can be 
    either an in-memory inverted index or any kind of database in the hard disk.
Hence, it is likely that the publicly provided in-memory storage functionality was not not used for their actual experiments,
    because they might have experienced the same memory issues.
Instead, probably they provided the \verb|code/inverted_index.py| file just for demonstration of their IR system's index
    storage functionality.
Unfortunately, the employed personal computer was equipped with just 16 GB 1600 MHz DDR3 random-access memory (RAM),
    and Google Colab only provides 12 GB RAM at maximum.
Only about 64,000 documents could be indexed and hold in-memory, although the whole collection contained 8,841,823 documents.
The imposed system hardware constraints led to a search for an alternative option of index storage.
Utilizing Numpy's \texttt{numpy.memmap}
    \footnote{Numpy documentation of \texttt{numpy.memmap} \url{https://numpy.org/doc/1.16/reference/generated/numpy.memmap.html?highlight=numpy.memmap\#numpy.memmap}} 
    function for creating a memory-map to an array stored in a binary file on disk, was the response to the 
    inefficient memory storage issue.
The modified inverted index storage comprises three datastructures.
Firstly, a Python dictionary \texttt{dict()} is used for establishing a mapping between a latent document representation's dimension
    to a list of document ids.
This datastructure is like an index, in returning all potentially relevant documents for a latent document representation's term.
Secondly, a Numpy two-dimensional memory-mapped file is used, which provides storage for the latent document representations' vectors
    of all intended documents for index construction.
Lastly, another Python dictionary is utilized for mapping a document id to its corresponding document representation's index within the memory-map array.
This datastructure can be used to retrieve the latent document representation for a given document id.
Both of these Python dictionary datastructures are stored using the Python function \texttt{pickle} on index construction completion,
    whereas the memory-mapped array is automatically stored on disk through Numpy.
If only a certain section of the memory mapped file is requested, Numpy loads only that part into memory, rather than loading the whole file from disk,
    which allows to index more documents and increases memory access.

After implementing the more sophisticated inverted index, the next challenge emerged.
Several hours and days were spent in training the model, index construction, retrieval and evaluation.
Indexing more than 1 Mio. documents was not feasible, because retrieval of documents for a single query sometimes took hours or even longer, so
    retrieval had to be terminated, because an end was not in sight.
The retrieval did not yield positive metrics for \verb|trec_eval| and manual observations showed that retrieval returned meaningless and irrelevant 
    document passages.\\
One attempt to cope with this issue was to write a new Python script \verb|code/generate_tokens.py| for generating a new token file, 
    containing all distinct tokens of the collection's documents in lower case, with an configurable occurence frequency 
    (e.g. token appears at least 10 times in the collection).
An analysis of one of the generated token files (\verb|"data/tokens/tokens_lowered_2019-12-28_001150_min_10.txt"|) showed,
    that the file holds 359,918 tokens, where one of the embeddings (\verb|glove.6B.100d.txt|) did not know 177,032 tokens.
That is, ca. 49\% of the tokens in the generated file were unknown tokens for the embedding,
    which means that for these unknown tokens a random vector is used as an embedding.
Training the model with the first 640,000 lines of the training dataset \verb|train.triples.tsv| and
    indexing 1 Mio. documents from the collection,
    by using this newly generated token file, unfortunately, yielded no improvement of the evaluation
    metrics.

To speed up evaluation two new Python scripts were written
    (\verb|code/qrels_subset.py|, \verb|code/query_subset_from_qrel.py|).
One of them creates out of an existing qrel file (e.g. \verb|qrels.dev.small.tsv|)
    a new subset qrel file containing only those queries, that have at least one 
    relevant document in the document collection that is intented to be indexed.
The other one creates for an existing query file (e.g. \verb|queries.dev.small.tsv|)
    and a subset qrel file a new query file, containing only the subset of queries from the input.
For experimenation in this early stage, sometimes only a subset of the documents from the 
    collection (e.g. the last 1 Mio.) were indexed, and hence, 
    the retrieval of queries that only have relevant documents in the qrel file, which are non-indexed,
    is useless.

The model was configured for these runs with three hidden layers, with 100, 80 and 100 units,
    and an output layer with 5,000 units.
After these unsuccessful attempts, another consideration was that potentially the model has not been trained
    long enough, and therefore the metrics are not positive.
Instead of training the model on the training dataset \verb|train.triples.tsv| over the first 640,000 entries,
    the model was trained over the first 2,259,200 rows.
To be more precise, the model was trained with 70,600 steps each time with a batch size of 32.
Training this model with the personal computer without a NVIDIA GPU available, took very long --- 
    approximately 17 hours.
Subsequent to index construction of the last 1 Mio. documents from the collection,
    the IR model did not find a single document for all evaluation queries,
    because the model inference returned for all query representations zero vectors.
Obviously, the vector representations were much too sparse to be useful.

% tensorboard diagramm siehe "2019-12-30 Tensorboard snrm-extension.pdf"
% TODO maybe add picture of tensorboard cost function converging to zero

Tensorboard was added for inspecting the model's training run.
According to one of the Tensorboard diagramms, the cost function during training
    was approaching towards 0, but after about 22,000 training steps the model
    swivel in to a value of 3 and stayed there.

After discussing the issue with the thesis advisor assistance, 
    a different document collection dataset (\verb|collection.air-subset.tsv|) 
    which is based on MS MARCO, but preprocessed for the Advanced Information Retrieval 
    lecture exercise, was chosen for experimentation.
This document collection should have a better ratio between relevant and non-relevant documents,
    than the previously utilized one.
For evaluation, as a query file, resp. qrels file the files \verb|queries.air-subset.tsv| resp.
    \verb|qrel.air-subset.tsv| were also downloaded from the university's OwnCloud server 
    \footnote{Advanced IR lecture exercise files (collection, query, qrel) \url{https://owncloud.tuwien.ac.at/index.php/s/1MugXnkP607Qm8P}}.
It was also tried to use as training dataset the MSMARCO \verb|triples.train.small.tsv| (27 GB)
    \footnote{MSMARCO passage ranking training set triples.train.small.tsv download link \url{https://msmarco.blob.core.windows.net/msmarcoranking/triples.train.small.tar.gz}}.
Due to Google Colab's hardware constraints of ca. free 30 GB disk space, rather than using the whole training,
    a smaller proportion of training triples (e.g. 6 GB) were used.

In another experiments, saving the model after every ten-thousand training steps, when training for about 70,000 steps, 
    showed, that, the longer the model had been trained, the more sparse the tensor representations became.
Either, the model was trained too long and the query representations had been the zero vector,
    or the query representation has non-zero values, however all documents are returned for such a query or 
    retrieval takes far too long to be feasible.
In other words, the IR systems either returns all documents from the collection as relevant or no document.
Obviously, the system was not able to learn and polarize different semantic concepts.

To better gauge SNRM during index construction and retrieval, Python logging was expanded,
    to log the sparsity of inferred query and document representations.
This showed useful to get a glimpse of how useful resulting representations are, and for deciding
    which models might not be worth for further experiments.
Especially for retrieval, besides sparsity, logging was also added for gaining knowledge of
    the number of existing documents per latent term dimension in the inverted index. \\
Tensorboard Logging was extended with diagrams of the number of zeros in tensors (sparsity) of 
    query and document representations, as well as the mean representations' values .
Aside from that, validation runs between model training phases, were separated from the Tensorboard logging 
    of training runs.

Experiments over multiple weeks were executed, trying to find a way to cope with the representations' sparsity 
    as the key problem.
Examination of the tests revealed, that the loss function approaches the value zero, 
    despite little variation, just after a few ten-thousand training steps.
The sparsity, the number of zeros, of the query and document representation tensors, similarly, 
    approaches soon 1, i.e. the representations coefficients tend to become all zero too fast.\\
Hyperparameter and the neural network architecture is configurable via the Python file \verb|"code/params.py"|.
In this configuration file, it is possible to easily adjust the number of neural network layers and neuron units
    for each layer separately, for instance.
Additionally, hyperparameters, like the learning rate, the convolution dropout rate, or SNRM's sparsity regulartization term
    are configurable.
Also, the maximum query and document length, which are used as the maximum length for trimming or padding tokenized text,
    are changeable using the config file.
Just to name a few modifications to SNRM's neural architecture during the test, the following number of layers and units per layer
    were attempted.
The labels "In" is the neural network input layer (embedding) and the number assigned is the number of neurons in this layer, 
    where "Out" is the output layer resp. query or document representation vector dimension.
The numbers between "In" and "Out" are the number of units per hidden layer. 
Hidden layers are separated by a dash ("-")
\begin{itemize}
    \item In: 300 -> 100 - 100 - 300 -> Out: 5000
    \item In: 300 -> 100 - 100 - 100 -> Out: 5000
    \item In: 300 -> 100 - 100 -> Out: 5000
    \item In: 300 -> 100 - 300 -> Out: 5000
    \item In: 300 -> 50 - 50 -> Out: 5000
\end{itemize}

Rather than using only an output dimension of 5,000, experiments were tried with 10,000 output neurons, too.
Unfortunately, the memory of the available local computers and on Google Colab was insufficient to create an inverted index
    with 10,000 dimensions.
Instead of using just a 100-dimensional GloVe embedding ()\verb|glove.6B.100d.txt|), it was also tried to 
    use a 300-dimensional word vectors (\verb|glove.42B.300d.txt|).
Related to the chosen embedding, it was moreover tried to either set the embedding trainable or not.
For configuring the query and document max length, a Python script was written to identify statistics of the 
    number of tokens per query and document passages in the training set and in the document collection, 
    like the mean and several quantiles
With these statistics the max query and document length was varied in multiple experiments.

Concerning hyperparameters, several tests were run with either different convoluion dropout rates or no dropout, 
    the learning rate was varied, and also the regularization term $\lambda$.\\
Zamani et al. employed a regularization parameter $\lambda$
    in the loss function, that should control the sparsity of the model's output representations.
In their paper, the authors revealed in a diagram an association between the value of the regularization term $\lambda$ and the 
    sparsity as well as the information retrieval evaluation metric "Mean average precision" (MAP).
Interestingly, in the author's diagram the representations do not become too fast too sparse.
For example,  with a regularization term of $\lambda=1*10^{-7}$, their representation sparsity is at 100,000 training steps 
    just at about $0.7 - 0.8$. \cite{zamani:2018:from-neural-reranking-to-neural-ranking}\\
Varying the regularization term $\lambda$ in the authors' endorsed interval of $[1*10^{-7}, 5*10^{-9}]$ and 
    even trying non-destined values, like 0, 30, or 1,000, but also negative values, did not change the course 
    of the representations' sparsity significantly in Tensorboard.

The following code shows a piece of the original SNRM model's cost function calculation.
The cost function is basically composed of the hinge loss, a L1 regularization reduces the coefficients of queries and 
    tuples of positive and negative document passages, and additionally, the effect of the regularization is
    controlled by the \texttt{regularization\_term}.

% TODO is this ok to use verbatim for code? or can we use something with syntax highlighting 
% or must we use the pseudocode-formatting like in intro.tex with \begin{algorithm}?
\begin{verbatim}
# the hinge loss function for training
self.loss = tf.losses.hinge_loss(
                logits=logits, 
                labels=self.labels_pl, 
                scope='hinge_loss', 
                reduction=tf.losses.Reduction.MEAN)

# the l1 regularization for sparsity. Since we use ReLU as 
# the activation function, all the outputs of the
# network are non-negative and thus we do not need to get 
the absolute value for computing the l1 loss.
self.l1_regularization = tf.reduce_mean(
    tf.reduce_sum(
        tf.concat(
            [self.q_repr, self.d1_repr, self.d2_repr], axis=1
        ), 
        axis=1),
    name='l1_regularization')

# the cost function including the hinge loss and the 
# l1 regularization.
self.cost = self.loss + 
    (tf.constant(self.regularization_term, dtype=tf.float32) * 
    self.l1_regularization)
\end{verbatim}

Although, trying these stated different modifications to the neural network architecture, 
    embedding, hyperparameters, and other configuration values,
    the plots in Tensorboard still looked quite similar to previous experiments.
The cost function was approaching the value 0 just after a few 10-thousand training steps.
Still, the query and document representations are getting too sparse too fast.
Especially, query representations tend to become more sparse earlier than document representations.
The latter is bad for retrieval, because no documents would be found for zero vector query representations, 
    despite having true positve coefficients in document representation vectors.
Therefore, the model training time was restricted due to the representations' rising sparsity,
    and as a result, the SNRM model could not deliver positive evaluation metrics.
% maybe add pictures of tensorboard diagramme 2020-01-15 Tensorboard snrm-extension.pdf 

A further challenge was, that Google Colab terminated sessions regulary, probably based on time spent or usage.
The occurence of these timeouts, led to moving the index construction, retrieval and evaluation to the 
    local available personal computer, and using Google Colab only for training the model GPU-accelerated.
Further delays have happened, because sometimes Google refused the usage of a compute instance equipped with a GPU,
    and therefore enforcing the utilization of a non-GPU system, 
    or to wait until a GPU machine becomes available again.

In the original SNRM code the Tensorflow model was designed with two-dimensional convolution layers, 
    followed by a ReLu (Rectified Linear Unit) activation function.
If configured also a dropout could be added after the activation function.\\
Since it was already tried in vain to use only the hinge loss without any regularization as the model's cost function,
    as a further attempt the model was modified additionally to ommit the dropout
    and to use only a ReLu activation layer on the last layer, i.e. on the output.
That experiment leaded to a similar result regarding sparsity, albeit with more a bit more variation in the cost function.
% maybe add image of 2020-01-24 tensorboard diagramme

The model experiments and results have been discussed with the advisor assistance, subsequentially.
Another proposed try was to adjust the tensor operation on the logits for the hinge loss,
    while keeping the previous neural network design, with only one ReLu activation function at the output layer and
    without using dropout.
In the original SNRM code a hinge loss function is used between given truth labels
    (i.e. determining which document is more relevant to a given query),
    and logits.
Originally, the logits were computed as a tensor multiplication (\texttt{tf.multiply}) of 
    a positve and a negative representation's batch and a query representation's batch,
    followed by a sum computation (\texttt{tf.reduce\_sum}), 
    and a concatenation of the two tensors (\texttt{tf.concat}).
The following code shows part of the original SNRM source code for the logits computation for the hinge loss.

% TODO is this ok to use verbatim for code? or can we use something with syntax highlighting 
% or must we use the pseudocode-formatting like in intro.tex with \begin{algorithm}?
\begin{verbatim}
logits_d1 = tf.reduce_sum(tf.multiply(self.q_repr,self.d1_repr),
                          axis=1, keep_dims=True)
logits_d2 = tf.reduce_sum(tf.multiply(self.q_repr,self.d2_repr),
                          axis=1, keep_dims=True)
logits = tf.concat([logits_d1, logits_d2], axis=1)

# the hinge loss function for training
self.loss = tf.reduce_mean(tf.losses.hinge_loss(
                                         logits=logits, 
                                         labels=self.labels_pl, 
                                         scope='hinge_loss'))
\end{verbatim}

In the course of this experiment, rather than using a sum reduction, it was tried to use
    a mean reduction (\texttt{tf.reduce\_mean}) instead for each document's logit computation.
Interestingly, without using a L1 regularization the representation sparsity had settled down after
    a few ten-thousand steps to about 0.8.
Compared to previous runs the sparsity approached 1.0 always, even without a L1 regularization utilized.
The cost function had started at not such a high values than before, but neared the value 1 much faster 
    and stays there.
Further experiments based on that were carried out, with the interest of adding a L1 regularization 
    and testing the controllability of the sparsity with the regularization term parameter $\lambda$.
Surprisingly, it was possible to change the query and documentation sparsity during training runs
    with different values for $\lambda$.
It was possible to increase the sparsity to a quite constant and desirable level of approx. 0.9.
When compared to previous attempts, the sparsity had not been controllable.
% add images of 2020-02-07 tensorboard diagramme.pdf















    
%     The format of a qrels file is as follows:
% TOPIC      ITERATION      DOCUMENT#      RELEVANCY 
% https://trec.nist.gov/data/qrels_eng/

% limitation of number of retrieval documents (max_retrieval_docs flag)
% retrieval.py statistics sparsity, logging of document count per latent term dimension found, logging empty retrieval list for query


