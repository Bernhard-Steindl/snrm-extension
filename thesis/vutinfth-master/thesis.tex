% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.

% ADDED by Bernhard Steindl:
\usepackage{listings} % Provides source code listings with styling - see https://ctan.org/pkg/listings

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Bernhard Steindl} % The author name without titles.
\newcommand{\thesistitle}{Standalone Neural Ranking Model (SNRM) with PyTorch} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {\thesistitle},              % The document's subject in the document properties (optional).
    pdfkeywords     = {neural information retrieval, information retrieval, information systems, ir, snrm, pytorch, tensorflow, msmarco} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

% TODO showing subsubsection in TOC is not default, should we change it or not?
\settocdepth{subsubsection} % Add subsubsections to TOC 

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{}{male}
\setadvisor{Univ. Prof. Dr.}{Allan Hanbury}{}{male}
 
% For bachelor and master theses:
\setfirstassistant{Univ. Ass. Dipl.-Ing.}{Sebastian Hofstätter}{BSc}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{01529136}
\setdate{01}{10}{2020} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
\setsubtitle{Extended for usage with the MS MARCO dataset}{Extended for usage with the MS MARCO dataset} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
\setthesis{bachelor}
%
% Master:
%\setthesis{master}
%\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Software \& Information Engineering}{Software \& Information Engineering} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

\frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
\addtitlepage{english} % English title page.
\addstatementpage

\begin{danksagung*}
Mein größter Dank gilt meinen lieben Eltern, Franz und Maria, die mich immer
    unterstützen und ermutigen, sowie mir meine Ausbildung ermöglicht haben.

Außerdem danke ich meinem Bruder Alexander, der oft mit guten Ratschlägen für
    mich da ist und mich im Studium bestärkt.

Meinem Betreuer für die Bachelorarbeit, Univ. Ass. Dipl.-Ing. Sebastian Hofstätter,
    danke ich ebenso für seine Zeit und für seine hilfreichen Anregungen für 
    meine Arbeit.

Darüberhinaus möchte ich Univ. Prof. Dr. Allan Hanbury und 
    Dipl.-Ing. Sebastian Hofstätter für einen interessanten 
    Einführungskurs zum Thema „Information Retrieval“ danken, 
    der mein Interesse an diesem Thema geweckt hat.
\end{danksagung*}

\begin{acknowledgements*}
My greatest thanks go to my dear parents, Franz and Maria, who always support 
    and encourage me and made my education possible. 

I also thank my brother Alexander, who is often there for me with good advice 
    and who encourages me in my studies.

My thanks also go to my supervisor assistance, Univ. Ass. Dipl.-Ing. Sebastian Hofstätter, 
    for his time and for his helpful suggestions for my thesis.

Furthermore, I would like to thank Univ. Prof. Dr. Allan Hanbury 
    and Dipl.-Ing. Sebastian Hofstätter for an interesting introductory 
    course on Information Retrieval which drew my interest in this topic.
\end{acknowledgements*}

\begin{kurzfassung}
Traditionelle „Information Retrieval“ (IR) Modelle, wie TF-IDF, verwenden Algorithmen, 
    die nur solche Dokumente berücksichtigen die eine exakte Übereinstimmung der 
    Wörter einer Suchanfrage enthalten, 
    um die Relevanz von Dokumenten für eine bestimmte Suchanfrage abzuschätzen.
Es ist jedoch möglich, dass Benutzer, die nach einem bestimmten Thema suchen, 
    ihr Suchinteresse nicht mit den gleichen Wörtern ausdrücken, 
    die in relevanten Dokumenten verwendet werden.
Infolgedessen sind die Benutzer möglicherweise nicht in der Lage, die Dokumente abzurufen, 
    die ihren Bedarf an Informationen decken.\\
Die aktuelle Forschung versucht, die Unterschiede im Vokabular zu kompensieren, 
    indem sie neuronale Netze in ihren „Information Retrieval“ Modellen einsetzt.
Neuronale „Information Retrieval“ Modelle könnten in der Lage sein, semantische Bedeutung 
    und Zusammenhänge zwischen Wörtern im Text zu erfassen und sind daher von Interesse.

In dieser Arbeit werden theoretische Aspekte zweier verschiedener Modelle, die 
    neuronale Netze verwenden, zusammengefasst und vorgestellt, 
    nämlich das „Neural Vector Space Model“ (NVSM) von Van Gysel et al. 
    \cite{van-gysel:2017:neural-vector-spaces} und 
    das „Standalone Neural Ranking Model“ (SNRM) von Zamani et al.
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}.\\
Der primäre Beitrag dieser Arbeit ist zum einen die Erweiterung und die
    Implementierung fehlender Funktionalität im originalen Quellcode von SNRM, 
    um die MS MARCO „Passage-Ranking“ Datensätzen von Microsoft
    für das Trainieren des Modells, für das Erstellen eines invertierten Index 
    und für das Abrufen von Dokumenten zu verwenden.
Darüber hinaus ist der zweite praktische Teil der Arbeit die Migration der 
    ursprünglichen bzw. erweiterten SNRM-Implementierung 
    von TensorFlow Version 1 zu einer aktuellen Version von PyTorch.\\
Im Laufe der Implementierung wurden Experimente durchgeführt mit der Absicht, 
    zu versuchen, die Ergebnisse der Abfrageevaluierung der Autoren zu reproduzieren.
Diese Bachelorarbeit beschreibt im Detail die Herausforderungen, 
    die sich ergeben haben, gewählte Lösungsansätze und Ergebnisse.
Trotz diverser Experimente, Anpassungen am neuronalen Netz der SNRM Architektur, 
    Variation von Hyperparametern und Konfigurationsparametern,
    zeigen die Ergebnisse, dass es schwierig ist, die 
    Spärlichkeit von erlernten Repräsentationen zu kontrollieren, 
    was die Effizienz bei der Erstellung eines invertierten Index einschränkt.
Leider war das „Standalone Neural Ranking Model“ nicht in der Lage 
    in den durchgeführten Experimenten semantische Beziehungen zwischen Wörtern lernen.
\end{kurzfassung}


% Abstract: Write your abstract in German and English in a length of half a page to 
% a full page and present the following aspects briefly and concisely: 
% the context of the work/task, the research question, the scientific method(s)/procedure(s) used 
% and the central results.
\begin{abstract}

Traditional information retrieval (IR) models such as TF-IDF use algorithms that 
    consider only those documents that contain an exact match of the words
    of a search query to 
    estimate the relevance of documents to a particular search query.
However, users searching for a specific topic might not express their 
    search interest with the same words used in relevant documents.
As a result, users might not be able to retrieve documents that satisfy
    their information needs.
Current research tries to bridge the vocabulary gap,
    by using neural networks in information retrieval models.
Neural information retrieval models might be able to capture semantic meaning and 
    relations between words in text and are therefore of interest.

This work summarizes and presents theoretic aspects of 
    two different neural network models, namely 
    the Neural Vector Space Model (NVSM) from Van Gysel et al. 
    \cite{van-gysel:2017:neural-vector-spaces}
    and 
    the Standalone Neural Ranking Model (SNRM) from Zamani et al.
    \cite{zamani:2018:from-neural-reranking-to-neural-ranking}. \\
The primary contribution of this work is on the one hand 
    the extension of and the implementation of missing functionality
    in the original SNRM's source code to be able to use
    Microsoft's MS MARCO passage ranking dataset for model training, for 
    creating an inverted index and for retrieval.
Additonally, the second practical part of the thesis is the 
    migration of the original resp. extended SNRM implementation
    from TensorFlow version 1 to an up-to-date version of PyTorch.\\
In the course of the implementation, experiments are carried out with
    the intention of trying to reproduce the authors' evaluation results.
This thesis describes in detail the challenges that have emerged,
    chosen solution approaches and results.
Despite diverse experiments, adjustments to the SNRM's neural network 
    architecture, variation of hyper parameters and configuration parameters,
    the results show, that it is difficult to control the sparsity of 
    the SNRM's learned representations, which limits the 
    efficiency of creating an inverted index.
Unfortunately, the Standalone Neural Ranking Model was not able to 
    learn semantic relations between words in the conducted experiments.
\end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
\selectlanguage{english}

% Add a table of contents (toc).
\tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
\mainmatter

% Introduction
\input{introduction.tex}

% Methodology
\input{methodology.tex}

% State of the Art
\input{state-of-the-art.tex}

% Design and Implementation
\input{design-and-implementation.tex}

% Conclusion and Outlook
\input{summary.tex}


\backmatter

% Use an optional list of figures.
\listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
% \cleardoublepage % Start list of tables on the next empty right hand page.
% \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of alogrithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
\printindex

% Add a glossary.
%\printglossaries

% Add a bibliography.
\bibliographystyle{alpha}
\bibliography{thesis}

\end{document}